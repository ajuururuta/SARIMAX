# -*- coding: utf-8 -*-

# ===============================================
# SARIMAX è‡ªåŠ¨å‚æ•°ä¼˜åŒ– + å¯è§†åŒ– + ä¸¤é˜¶æ®µå¹¶è¡Œæœç´¢ + ä¼˜åŒ–ç‰ˆ
# ===============================================
# ä¼˜åŒ–ç‚¹ï¼š
# - è§£å†³æ‰¹æ¬¡è¶…æ—¶å’Œå¤±è´¥é—®é¢˜
# - åŠ é€Ÿç²—ç­›é˜¶æ®µï¼ˆæ—©åœã€å¿«é€Ÿæ¨¡å¼ã€ä¼˜åŒ–å¹¶è¡Œï¼‰
# - GPU æ£€æµ‹å’Œä¼˜åŒ–å»ºè®®
# - è¿›åº¦ä¿å­˜å’Œæ¢å¤
# - æ”¹è¿›çš„é”™è¯¯å¤„ç†å’Œè¶…æ—¶æ§åˆ¶
# ===============================================

import warnings
warnings.filterwarnings('ignore')

import os
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
import pickle
from pathlib import Path

from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose

from sklearn.metrics import mean_squared_error

from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing as mp

from statsmodels.tools.sm_exceptions import ConvergenceWarning
warnings.filterwarnings('ignore', category=ConvergenceWarning)

# ============== GPU æ£€æµ‹ ==============
def check_gpu_available():
    """æ£€æµ‹ GPU å’Œç›¸å…³åŠ é€Ÿåº“"""
    gpu_info = {
        'cuda_available': False,
        'cupy_available': False,
        'mkl_available': False,
        'openblas_available': False
    }
    
    try:
        import cupy as cp
        gpu_info['cupy_available'] = True
        gpu_info['cuda_available'] = True
        print(f"âœ“ æ£€æµ‹åˆ° CuPy (GPU åŠ é€Ÿå¯ç”¨)")
    except ImportError:
        print("âœ— CuPy æœªå®‰è£… (pip install cupy-cuda11x æˆ– cupy-cuda12x)")
    
    # æ£€æµ‹ BLAS åº“
    try:
        import numpy as np
        config = np.__config__.show()
        if 'mkl' in str(config).lower():
            gpu_info['mkl_available'] = True
            print("âœ“ æ£€æµ‹åˆ° Intel MKL (CPU ä¼˜åŒ–)")
        elif 'openblas' in str(config).lower():
            gpu_info['openblas_available'] = True
            print("âœ“ æ£€æµ‹åˆ° OpenBLAS (CPU ä¼˜åŒ–)")
    except:
        pass
    
    return gpu_info

# ============== å‚æ•°è§£æ ============== 
def _env_int(name, default):
    try:
        return int(os.environ.get(name, default))
    except Exception:
        return default


def _env_float(name, default):
    try:
        return float(os.environ.get(name, default))
    except Exception:
        return default


def _get_n_jobs():
    try:
        n = int(os.environ.get('SARIMAX_N_JOBS', '-1'))
        if n == -1:
            return max(1, mp.cpu_count() - 1)  # ç•™ä¸€ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
        return n
    except Exception:
        return max(1, mp.cpu_count() - 1)


def _ensure_blas_single_thread():
    """è®¾ç½® BLAS åº“ä¸ºå•çº¿ç¨‹ï¼Œé¿å…è¿‡åº¦è®¢é˜…"""
    for var in ["OMP_NUM_THREADS", "MKL_NUM_THREADS", "NUMEXPR_NUM_THREADS", "OPENBLAS_NUM_THREADS"]:
        if os.environ.get(var) is None:
            os.environ[var] = "1"

_ensure_blas_single_thread()

parser = argparse.ArgumentParser(description="SARIMAX ä¸¤é˜¶æ®µæœç´¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰")
parser.add_argument("--coarse-maxiter", type=int, default=_env_int("COARSE_MAXITER", 100), help="ç²—ç­›æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ 100ï¼ˆé™ä½ä»¥åŠ é€Ÿï¼‰")
parser.add_argument("--fine-maxiter",   type=int, default=_env_int("FINE_MAXITER", 500), help="ç²¾è°ƒæœ€å¤§è¿­ä»£æ¬¡ï¿½ï¿½ï¿½ï¼Œé»˜è®¤ 500")
parser.add_argument("--top-k",          type=int, default=_env_int("TOP_K", 10), help="ç²—ç­›ä¿ç•™çš„å‰ K ä¸ªæ¨¡å‹ï¼Œé»˜è®¤ 10")
parser.add_argument("--small-gap",      type=float, default=_env_float("SMALL_GAP", 0.5), help="AIC å·®è·é˜ˆå€¼ï¼Œé»˜è®¤ 0.5")
parser.add_argument("--no-fine-expand", action="store_true", help="ç¦ç”¨ç²¾è°ƒé˜¶æ®µçš„å±€éƒ¨æ‰©å±•ç½‘æ ¼")
parser.add_argument("--quick-mode",     action="store_true", help="å¿«é€Ÿæ¨¡å¼ï¼šå‡å°‘æœç´¢ç©ºé—´")
parser.add_argument("--early-stop",     type=int, default=5, help="æ—©åœï¼šè¿ç»­ N ä¸ªæ‰¹æ¬¡æ— æ”¹å–„åˆ™åœæ­¢ï¼Œé»˜è®¤ 5ï¼Œ0 ä¸ºç¦ç”¨")

parser.add_argument("--resume",         action="store_true", help="ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤")
parser.add_argument("--backend",        type=str, default="threading", choices=["loky", "threading", "multiprocessing"], help="å¹¶è¡Œåç«¯ï¼Œé»˜è®¤ threadingï¼ˆæ›´å¿«å¯åŠ¨ï¼‰")
args, _ = parser.parse_known_args()

COARSE_MAXITER = args.coarse_maxiter
FINE_MAXITER   = args.fine_maxiter
TOP_K          = args.top_k
SMALL_GAP      = args.small_gap
FINE_EXPAND    = not args.no_fine_expand
QUICK_MODE     = args.quick_mode
EARLY_STOP_BATCHES = args.early_stop

RESUME         = args.resume
BACKEND        = args.backend

print("="*60)
print("SARIMAX ä¼˜åŒ–ç‰ˆå¯åŠ¨")
print("="*60)
gpu_info = check_gpu_available()
print(f"[å‚æ•°] COARSE_MAXITER={COARSE_MAXITER}, FINE_MAXITER={FINE_MAXITER}")
print(f"[å‚æ•°] TOP_K={TOP_K}, SMALL_GAP={SMALL_GAP}, FINE_EXPAND={FINE_EXPAND}")
print(f"[å‚æ•°] QUICK_MODE={QUICK_MODE}, EARLY_STOP={EARLY_STOP_BATCHES}")
print(f"[å‚æ•°] æ— è¶…æ—¶é™åˆ¶ - ç¡®ä¿æ¯ä¸ªæ¨¡å‹å®Œæ•´æ‹Ÿåˆ")
print(f"[å‚æ•°] BACKEND={BACKEND}, N_JOBS={_get_n_jobs()}")
print("="*60)

plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

# ============== æ•°æ®è¯»å–ä¸é¢„å¤„ç† ============== 
print("\n[1/6] æ•°æ®åŠ è½½...")
raw = pd.read_csv('data.csv')
if 'timestamp' in raw.columns:
    raw['timestamp'] = pd.to_datetime(raw['timestamp'])
    raw = raw.set_index('timestamp')
else:
    raw.iloc[:, 0] = pd.to_datetime(raw.iloc[:, 0])
    raw = raw.set_index(raw.columns[0])

raw = raw.sort_index()
raw = raw.asfreq('T')

missing_count = raw['VALUE'].isna().sum()
if missing_count > 0:
    print(f'  å‘ç°ç¼ºå¤±å€¼ {missing_count} ä¸ªï¼Œä½¿ç”¨çº¿æ€§æ’å€¼ä¿®å¤')
    raw['VALUE'] = raw['VALUE'].interpolate()

if raw.index.duplicated().any():
    print('  å‘ç°é‡å¤æ—¶é—´æˆ³ï¼ŒæŒ‰å‡å€¼èšåˆ')
    raw = raw.groupby(raw.index).mean()
    raw = raw.asfreq('T')

print(f'  æ•°æ®èŒƒå›´: {raw.index.min()} -> {raw.index.max()}')
print(f'  æ€»æ ·æœ¬æ•°: {len(raw)}')

data = raw.copy()

# ============== å¹³ç¨³æ€§æ£€éªŒ ============== 
print("\n[2/6] å¹³ç¨³æ€§æ£€éªŒ...")
adf_stat, adf_p, *_ = adfuller(data['VALUE'])
print(f'  ADF Statistic: {adf_stat:.4f}, p-value: {adf_p:.6g}')
if adf_p < 0.05:
    d = 0
    print('  âœ“ åºåˆ—å·²å¹³ç¨³ï¼Œd=0')
else:
    d = 1
    print('  âœ— åºåˆ—ä¸å¹³ç¨³ï¼Œä½¿ç”¨ d=1')

# ============== æœç´¢ç©ºé—´ ============== 
print("\n[3/6] æ„å»ºæœç´¢ç©ºé—´...")
if QUICK_MODE:
    print("  [å¿«é€Ÿæ¨¡å¼] ä½¿ç”¨ç²¾ç®€æœç´¢ç©ºé—´")
    p_range = range(0, 2)
    q_range = range(0, 2)
    P_range = range(0, 2)
    Q_range = range(0, 2)
    D_values = [0, 1]
else:
    p_range = range(0, 3)
    q_range = range(0, 3)
    P_range = range(0, 2)
    Q_range = range(0, 2)
    D_values = [0, 1]

s_candidates = [s for s in ([60, 120] if len(data) >= 120 else [60]) if len(data) >= 3*s]
if not s_candidates:
    s_candidates = [60]
print(f'  å­£èŠ‚å‘¨æœŸå€™é€‰: {s_candidates}')

param_grid = [(p, d, q, P, D_, Q, s_) for p in p_range for q in q_range
              for P in P_range for Q in Q_range for D_ in D_values for s_ in s_candidates]
print(f'  æ€»å‚æ•°ç»„åˆ: {len(param_grid)}')

# ============== æ‹Ÿåˆå‡½æ•°ï¼ˆæ— è¶…æ—¶é™åˆ¶ï¼‰ ============== 
def _fit_robust_no_timeout(model, maxiter=200, stage='coarse'):
    """é²æ£’æ‹Ÿåˆï¼Œæ— è¶…æ—¶é™åˆ¶ï¼Œå°è¯•å¤šç§ä¼˜åŒ–å™¨"""
    methods_coarse = ['lbfgs', 'powell', 'bfgs']
    methods_fine = ['lbfgs', 'powell', 'nm', 'bfgs', 'cg']
    methods = methods_coarse if stage == 'coarse' else methods_fine
    
    for method_idx, m in enumerate(methods):
        try:
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore')
                actual_maxiter = maxiter if method_idx == 0 else maxiter * 2
                res = model.fit(method=m, disp=False, maxiter=actual_maxiter)
            
            if bool(res.mle_retvals.get('converged', True)) and np.isfinite(res.aic):
                return res
        except KeyboardInterrupt:
            raise
        except Exception as e:
            continue
    
    return None


def _build_model(endog, order, seasonal_order, simple_diff=False):
    """æ„å»º SARIMAX æ¨¡å‹"""
    try:
        return SARIMAX(
            endog, order=order, seasonal_order=seasonal_order,
            enforce_stationarity=True, enforce_invertibility=True,
            simple_differencing=simple_diff
        )
    except Exception as e:
        try:
            return SARIMAX(
                endog, order=order, seasonal_order=seasonal_order,
                enforce_stationarity=False, enforce_invertibility=False,
                simple_differencing=simple_diff
            )
        except:
            return None


def _fit_one_stage(endog, order, seasonal_order, maxiter=200, stage='coarse'):
    """å•ä¸ªæ¨¡å‹æ‹Ÿåˆï¼ˆå¢å¼ºç‰ˆï¼Œå¤šé‡å›é€€æœºåˆ¶ï¼‰"""
    try:
        # ç¬¬ä¸€æ¬¡å°è¯•ï¼šæ ‡å‡†æ‹Ÿåˆ
        model = _build_model(endog, order, seasonal_order, simple_diff=False)
        if model is None:
            return None
            
        res = _fit_robust_no_timeout(model, maxiter=maxiter, stage=stage)
        
        # ç¬¬äºŒæ¬¡å°è¯•ï¼šå¦‚æœå¤±è´¥ä¸”æœ‰å·®åˆ†ï¼Œå°è¯• simple_differencing
        if res is None and (order[1] > 0 or seasonal_order[1] > 0):
            model_sd = _build_model(endog, order, seasonal_order, simple_diff=True)
            if model_sd is not None:
                res = _fit_robust_no_timeout(model_sd, maxiter=maxiter, stage=stage)
        
        # ç¬¬ä¸‰æ¬¡å°è¯•ï¼šå¦‚æœä»å¤±è´¥ï¼Œå°è¯•æ›´å¤šè¿­ä»£æ¬¡æ•°
        if res is None:
            model_more_iter = _build_model(endog, order, seasonal_order, simple_diff=False)
            if model_more_iter is not None:
                res = _fit_robust_no_timeout(model_more_iter, maxiter=maxiter*3, stage=stage)
        
        if res is None or not np.isfinite(res.aic):
            return None
        
        return {
            'p': order[0], 'd': order[1], 'q': order[2],
            'P': seasonal_order[0], 'D': seasonal_order[1], 'Q': seasonal_order[2], 's': seasonal_order[3],
            'AIC': float(res.aic)
        }
    except KeyboardInterrupt:
        raise
    except Exception as e:
        return None


def _clip(v, low, high):
    return max(low, min(high, v))


def _make_local_grid(base_rows, p_max, q_max, P_max, Q_max):
    local = set()
    for _, row in base_rows.iterrows():
        p0, d0, q0 = int(row.p), int(row.d), int(row.q)
        P0, D0, Q0, s0 = int(row.P), int(row.D), int(row.Q), int(row.s)
        for dp in [-1, 0, 1]:
            for dq in [-1, 0, 1]:
                for dP in [-1, 0, 1]:
                    for dQ in [-1, 0, 1]:
                        p_new = _clip(p0 + dp, 0, p_max)
                        q_new = _clip(q0 + dq, 0, q_max)
                        P_new = _clip(P0 + dP, 0, P_max)
                        Q_new = _clip(Q0 + dQ, 0, Q_max)
                        local.add((p_new, d0, q_new, P_new, D0, Q_new, s0))
    return list(local)

# ============== æ”¹è¿›çš„å¹¶è¡Œæœç´¢ ============== 
def parallel_search_optimized(endog, grid, n_jobs, maxiter, stage_desc, stage_key, 
                              early_stop_batches=5, resume_file=None):
    """ä¼˜åŒ–çš„å¹¶è¡Œæœç´¢ï¼Œæ”¯æŒæ—©åœå’Œæ¢å¤"""
    
    tasks = [((p, d_, q), (P, D_, Q, s_)) for (p, d_, q, P, D_, Q, s_) in grid]
    print(f'\n{stage_desc}')
    print(f'  ä»»åŠ¡æ€»æ•°: {len(tasks)}, å¹¶è¡Œåº¦: {n_jobs}, maxiter: {maxiter}')
    print(f'  æ—©åœæ‰¹æ¬¡: {early_stop_batches}')
    print(f'  æ— è¶…æ—¶é™åˆ¶ - æ¯ä¸ªæ¨¡å‹å°†å®Œæ•´æ‹Ÿåˆç›´åˆ°æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°')
    
    # æ¢å¤ä¹‹å‰çš„ç»“æœ
    completed_tasks = set()
    all_results = []
    
    if resume_file and os.path.exists(resume_file):
        try:
            with open(resume_file, 'rb') as f:
                checkpoint = pickle.load(f)
                all_results = checkpoint['results']
                completed_tasks = checkpoint['completed']
            print(f'  âœ“ ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œå·²å®Œæˆ {len(completed_tasks)} ä¸ªä»»åŠ¡')
        except Exception as e:
            print(f'  âœ— æ¢å¤å¤±è´¥: {e}')
    
    # è¿‡æ»¤å·²å®Œæˆçš„ä»»åŠ¡
    remaining_tasks = [t for i, t in enumerate(tasks) if i not in completed_tasks]
    
    if not remaining_tasks:
        print('  âœ“ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ')
        if not all_results:
            return pd.DataFrame(columns=['p','d','q','P','D','Q','s','AIC'])
        return pd.DataFrame(all_results).sort_values('AIC').reset_index(drop=True)
    
    best_aic = min([r['AIC'] for r in all_results]) if all_results else float('inf')
    no_improvement_count = 0
    
    # åŠ¨æ€æ‰¹æ¬¡å¤§å°ï¼šå¼€å§‹æ—¶å°æ‰¹æ¬¡ï¼ŒåæœŸå¤§æ‰¹æ¬¡
    batch_size = max(4, len(remaining_tasks) // 20)
    
    start_time = time.time()
    
    with tqdm(total=len(remaining_tasks), desc=f"  {stage_desc}", ncols=100, unit="æ¨¡å‹") as pbar:
        for batch_idx, i in enumerate(range(0, len(remaining_tasks), batch_size)):
            batch_tasks = remaining_tasks[i:i+batch_size]
            batch_start = time.time()
            
            try:
                # ä½¿ç”¨æŒ‡å®šçš„åç«¯ï¼Œæ— è¶…æ—¶é™åˆ¶
                batch_results = Parallel(
                    n_jobs=n_jobs, 
                    backend=BACKEND, 
                    timeout=None
                )(
                    delayed(_fit_one_stage)(endog, order, seasonal, maxiter, stage_key)
                    for order, seasonal in batch_tasks
                )
                
                # è¿‡æ»¤æœ‰æ•ˆç»“æœ
                valid_batch = [r for r in batch_results if r is not None]
                all_results.extend(valid_batch)
                
                # æ›´æ–°æœ€ä¼˜ AIC å’Œæ—©åœè®¡æ•°
                batch_improved = False
                if valid_batch:
                    batch_best = min(r['AIC'] for r in valid_batch)
                    if batch_best < best_aic - 0.01:  # è‡³å°‘æ”¹å–„ 0.01
                        best_aic = batch_best
                        no_improvement_count = 0
                        batch_improved = True
                    else:
                        no_improvement_count += 1
                else:
                    no_improvement_count += 1
                batch_time = time.time() - batch_start
                
                # æ›´æ–°è¿›åº¦æ¡
                success_rate = len(valid_batch) / len(batch_tasks) * 100 if batch_tasks else 0
                eta_per_task = (time.time() - start_time) / (i + len(batch_tasks))
                eta_remaining = eta_per_task * (len(remaining_tasks) - i - len(batch_tasks))
                
                pbar.set_postfix_str(
                    f"AIC: {best_aic:.2f} | æˆåŠŸç‡: {success_rate:.0f}% | æ‰¹æ¬¡è€—æ—¶: {batch_time:.1f}s | ETA: {eta_remaining/60:.1f}min"
                )
                pbar.update(len(batch_tasks))
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if resume_file and (batch_idx + 1) % 5 == 0:
                    completed_tasks.update(range(i, i + len(batch_tasks)))
                    with open(resume_file, 'wb') as f:
                        pickle.dump({
                            'results': all_results,
                            'completed': completed_tasks
                        }, f)
                
                # æ—©åœæ£€æŸ¥
                if early_stop_batches > 0 and no_improvement_count >= early_stop_batches:
                    print(f'\n  âš  æ—©åœè§¦å‘ï¼šè¿ç»­ {early_stop_batches} ä¸ªæ‰¹æ¬¡æ— æ˜¾è‘—æ”¹å–„')
                    break
                
            except KeyboardInterrupt:
                print(f'\n  âš  ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦...')
                # ä¿å­˜è¿›åº¦
                if resume_file:
                    completed_tasks.update(range(i, i + len(batch_tasks)))
                    with open(resume_file, 'wb') as f:
                        pickle.dump({
                            'results': all_results,
                            'completed': completed_tasks
                        }, f)
                    print(f'  âœ“ è¿›åº¦å·²ä¿å­˜åˆ° {resume_file}')
                raise
            except Exception as e:
                print(f'\n  âœ— æ‰¹æ¬¡ {i}-{i+len(batch_tasks)} å¤±è´¥: {str(e)[:100]}')
                # å¤±è´¥æ—¶å°è¯•å•ä¸ªä»»åŠ¡å¤„ç†
                for task in batch_tasks:
                    try:
                        single_result = _fit_one_stage(endog, task[0], task[1], maxiter, stage_key)
                        if single_result:
                            all_results.append(single_result)
                    except KeyboardInterrupt:
                        raise
                    except:
                        pass
                pbar.update(len(batch_tasks))
    
    total_time = time.time() - start_time
    print(f'  å®Œæˆæ—¶é—´: {total_time/60:.2f} åˆ†é’Ÿ')
    print(f'  æˆåŠŸæ¨¡å‹: {len(all_results)} / {len(remaining_tasks)} ({len(all_results)/len(remaining_tasks)*100:.1f}%)')
    
    # æ¸…ç†æ£€æŸ¥ç‚¹
    if resume_file and os.path.exists(resume_file):
        try:
            os.remove(resume_file)
        except:
            pass
    
    if not all_results:
        return pd.DataFrame(columns=['p','d','q','P','D','Q','s','AIC'])
    return pd.DataFrame(all_results).sort_values('AIC').reset_index(drop=True)

# ============== ä¸¤é˜¶æ®µæœç´¢ ============== 
TEST_STEPS = 100
train = data.iloc[:-TEST_STEPS]
N_JOBS = _get_n_jobs()

print('\n[4/6] å‚æ•°æœç´¢')
print('='*60)
print('é˜¶æ®µ 1: ç²—ç­›')

resume_coarse = 'checkpoint_coarse.pkl' if RESUME else None
coarse_df = parallel_search_optimized(
    train['VALUE'], param_grid, N_JOBS, COARSE_MAXITER, 
    'ç²—ç­›æœç´¢', 'coarse',
    early_stop_batches=EARLY_STOP_BATCHES,
    resume_file=resume_coarse
)

print('\n  å‰ 10 ä¸ªç»“æœ:')
print(coarse_df.head(min(10, len(coarse_df))).to_string())

if coarse_df.empty:
    raise RuntimeError('ç²—ç­›é˜¶æ®µæ²¡æœ‰æ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–æ”¾å®½å‚æ•°çº¦æŸ')

# åˆ¤æ–­æ˜¯å¦éœ€è¦ç²¾è°ƒ
need_fine = True
if len(coarse_df) >= TOP_K:
    gap = coarse_df.iloc[TOP_K-1].AIC - coarse_df.iloc[0].AIC
    print(f'\n  AIC å·®è· (ç¬¬1 vs ç¬¬{TOP_K}): {gap:.4f}')
    if gap < SMALL_GAP:
        print(f'  âœ“ AIC å·®è· < {SMALL_GAP}ï¼Œè·³è¿‡ç²¾è°ƒ')
        need_fine = False
else:
    print(f'\n  å¯ç”¨æ¨¡å‹ < TOP_K={TOP_K}ï¼Œè·³è¿‡ç²¾è°ƒ')
    need_fine = False

fine_df = None
if need_fine and FINE_EXPAND:
    print('\n' + '='*60)
    print('é˜¶æ®µ 2: ç²¾è°ƒ')
    base_top = coarse_df.head(TOP_K)
    local_grid = _make_local_grid(base_top, max(p_range), max(q_range), max(P_range), max(Q_range))
    coarse_set = set(param_grid)
    local_grid = [g for g in local_grid if g in coarse_set]
    print(f'  å±€éƒ¨æ‰©å±•ç½‘æ ¼å¤§å°: {len(local_grid)}')
    
    if local_grid:
        resume_fine = 'checkpoint_fine.pkl' if RESUME else None
        fine_df = parallel_search_optimized(
            train['VALUE'], local_grid, N_JOBS, FINE_MAXITER,
            'ç²¾è°ƒæœç´¢', 'fine',
            early_stop_batches=max(3, EARLY_STOP_BATCHES // 2),
            resume_file=resume_fine
        )
        print('\n  å‰ 5 ä¸ªç²¾è°ƒç»“æœ:')
        print(fine_df.head(min(5, len(fine_df))).to_string())
    else:
        print('  å±€éƒ¨æ‰©å±•ç½‘æ ¼ä¸ºç©º')
        need_fine = False

# åˆå¹¶ç»“æœ
if need_fine and fine_df is not None and not fine_df.empty:
    best_all = pd.concat([coarse_df.head(TOP_K), fine_df], ignore_index=True)
    best_all = best_all.sort_values('AIC').reset_index(drop=True)
    final_row = best_all.iloc[0]
    source = 'ç²¾è°ƒ'
else:
    final_row = coarse_df.iloc[0]
    source = 'ç²—ç­›'

print('\n' + '='*60)
print(f'æœ€ä¼˜å‚æ•° (æ¥æº: {source}):')
print(final_row.to_string())
print('='*60)

final_order = (int(final_row.p), int(final_row.d), int(final_row.q))
final_seasonal = (int(final_row.P), int(final_row.D), int(final_row.Q), int(final_row.s))

# ============== æœ€ç»ˆæ¨¡å‹æ‹Ÿåˆ ============== 
print('\n[5/6] æœ€ç»ˆæ¨¡å‹æ‹Ÿåˆ...')
final_model = _build_model(data['VALUE'], final_order, final_seasonal, simple_diff=False)
if final_model is None:
    raise RuntimeError('æœ€ç»ˆæ¨¡å‹æ„å»ºå¤±è´¥')
final_res = _fit_robust_no_timeout(final_model, maxiter=max(500, FINE_MAXITER), stage='fine')

if final_res is None and (final_order[1] > 0 or final_seasonal[1] > 0):
    print('  å°è¯• simple_differencing å›é€€...')
    final_model_sd = _build_model(data['VALUE'], final_order, final_seasonal, simple_diff=True)
    if final_model_sd is not None:
        final_res = _fit_robust_no_timeout(final_model_sd, maxiter=600, stage='fine')

if final_res is None:
    raise RuntimeError('æœ€ç»ˆæ¨¡å‹æ‹Ÿåˆå¤±è´¥ï¼Œè¯·å°è¯•è°ƒæ•´å‚æ•°æˆ–æ£€æŸ¥æ•°æ®è´¨é‡')

print('  âœ“ æ¨¡å‹æ‹ŸåˆæˆåŠŸ')
print(final_res.summary())

# ============== é¢„æµ‹è¯„ä¼° ============== 
print('\n[6/6] é¢„æµ‹ä¸è¯„ä¼°...')
forecast = final_res.get_forecast(steps=TEST_STEPS)
forecast_mean = forecast.predicted_mean
forecast_ci = forecast.conf_int(alpha=0.05)
actual_test = data['VALUE'].iloc[-TEST_STEPS:]
mse = mean_squared_error(actual_test, forecast_mean)
rmse = np.sqrt(mse)
mae = np.mean(np.abs(actual_test - forecast_mean))

print(f'  æµ‹è¯•é›† MSE: {mse:.4f}')
print(f'  æµ‹è¯•é›† RMSE: {rmse:.4f}')
print(f'  æµ‹è¯•é›† MAE: {mae:.4f}')

# ============== å¯è§†åŒ– ============== 
print('\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...')

plt.figure(figsize=(12,4))
plt.plot(data['VALUE'], label='åŸå§‹åºåˆ—', alpha=0.8)
plt.title('åŸå§‹æ—¶é—´åºåˆ—')
plt.legend()
plt.tight_layout()

if d == 1:
    plt.figure(figsize=(12,4))
    plt.plot(data['VALUE'].diff(), label='ä¸€æ¬¡å·®åˆ†', alpha=0.8)
    plt.title('å·®åˆ†ååºåˆ— (d=1)')
    plt.legend()
    plt.tight_layout()

fig, ax = plt.subplots(1,2, figsize=(14,4))
plot_acf(data['VALUE'].diff().dropna() if d==1 else data['VALUE'], ax=ax[0], lags=min(60, len(data)//3))
ax[0].set_title('è‡ªç›¸å…³å‡½æ•° (ACF)')
plot_pacf(data['VALUE'].diff().dropna() if d==1 else data['VALUE'], ax=ax[1], lags=min(60, len(data)//3), method='ywm')
ax[1].set_title('åè‡ªç›¸å…³å‡½æ•° (PACF)')
plt.tight_layout()

try:
    decomp = seasonal_decompose(data['VALUE'], period=final_seasonal[3], model='additive', extrapolate_trend='freq')
    decomp.plot()
    plt.suptitle(f'å­£èŠ‚åˆ†è§£ (å‘¨æœŸ={final_seasonal[3]})', y=1.02)
    plt.tight_layout()
except Exception as e:
    print(f'  å­£èŠ‚åˆ†è§£å¤±è´¥: {e}')  

plt.figure(figsize=(12,5))
plt.plot(data['VALUE'], label='å®é™…å€¼', alpha=0.7)
plt.plot(final_res.fittedvalues, label='æ‹Ÿåˆå€¼', alpha=0.7)
plt.title('æ¨¡å‹æ‹Ÿåˆæ•ˆæœ')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()

plt.figure(figsize=(12,5))
plt.plot(data.index, data['VALUE'], label='å†å²æ•°æ®', alpha=0.7)
future_index = pd.date_range(start=data.index[-1] + pd.Timedelta(minutes=1), periods=TEST_STEPS, freq='T')
plt.plot(future_index, forecast_mean, label='é¢„æµ‹å€¼', color='red', linewidth=2)
plt.fill_between(future_index, forecast_ci.iloc[:,0], forecast_ci.iloc[:,1], color='lightcoral', alpha=0.3, label='95% ç½®ä¿¡åŒºé—´')
plt.axvline(x=data.index[-TEST_STEPS], color='gray', linestyle='--', alpha=0.5)
plt.title(f'{TEST_STEPS} æ­¥é¢„æµ‹ç»“æœ')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()

try:
    final_res.plot_diagnostics(figsize=(12,8))
    plt.suptitle('æ®‹å·®è¯Šæ–­å›¾', y=1.00)
    plt.tight_layout()
except Exception as e:
    print(f'  æ®‹å·®è¯Šæ–­å›¾å¤±è´¥: {e}')  

plt.show()

# ============== ç»“æœæ€»ç»“ ============== 
print('\n' + '='*60)
print('æ¨¡å‹é€‰æ‹©ä¸ç»“æœæ€»ç»“')
print('='*60)
print(f'æœ€ä¼˜éå­£èŠ‚å‚æ•° (p,d,q): {final_order}')
print(f'æœ€ä¼˜å­£èŠ‚å‚æ•° (P,D,Q,s): {final_seasonal}')
print(f'æœ€ä¼˜ AIC: {final_row.AIC:.2f}')
print(f'æµ‹è¯•é›† MSE: {mse:.4f}')
print(f'æµ‹è¯•é›† RMSE: {rmse:.4f}')
print(f'æµ‹è¯•é›† MAE: {mae:.4f}')
print(f'\né…ç½®å‚æ•°:')
print(f'  COARSE_MAXITER={COARSE_MAXITER}')
print(f'  FINE_MAXITER={FINE_MAXITER}')
print(f'  TOP_K={TOP_K}')
print(f'  SMALL_GAP={SMALL_GAP}')
print(f'  QUICK_MODE={QUICK_MODE}')
print(f'  EARLY_STOP={EARLY_STOP_BATCHES}')
print(f'  BACKEND={BACKEND}')
print(f'  N_JOBS={N_JOBS}')
print(f'  æ— è¶…æ—¶é™åˆ¶ - æ‰€æœ‰æ¨¡å‹å®Œæ•´æ‹Ÿåˆ')
print('='*60)

if not gpu_info['cuda_available']:
    print('\nğŸ’¡ ä¼˜åŒ–å»ºè®®:')
    print('  1. å®‰è£… CuPy ä»¥åˆ©ç”¨ GPU åŠ é€Ÿæ•°æ®é¢„å¤„ç†:')
    print('     pip install cupy-cuda11x  # æˆ– cupy-cuda12x')
    print('  2. ä½¿ç”¨ --quick-mode å‡å°‘æœç´¢ç©ºé—´')
    print('  3. è°ƒæ•´ --early-stop å‚æ•°å®ç°æ›´å¿«çš„æ”¶æ•›')
    print('  4. ä½¿ç”¨ --coarse-maxiter 50 è¿›ä¸€æ­¥åŠ é€Ÿç²—ç­›')

print('\nâœ“ ç¨‹åºæ‰§è¡Œå®Œæˆï¼')
